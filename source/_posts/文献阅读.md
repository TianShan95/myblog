## TEVC
## 2022-Evolutionary Search for Complete Neural Network Architectures With Partial Weight Sharing
### 优劣分析
1. However, the full weight sharing training paradigm in OSNAS may result in strong interference across candidate architectures and mislead the architecture search. #局限-权重继承
2. The efﬁciency of performance estimation of the candidate architectures is therefore greatly enhanced because it avoids training a large number of candidate models from scratch.
   oneshot可以不用反复训练的优势。我的论文基于块的节点继承，大部分研究为了减少评估运算量使用的是整个 #优势-权重继承

### 总结
1. 虽然本文应用了oneshot，本文的搜索空间涉及不同的操作类型，<mark>每一层的都固定一种类型的的操作，每次选择时选中，则直接继承，从而实现oneshot，但是一开始无法训练所有情况的模块，则需要设计一个模块库。交叉我们也可以使用两阶段，GNN层面和layer层面。然后变异根据库内的情况，增加个体多样性。</mark>
2. oneshot网络
3. 有结构基因和switch基因两种
4. 演化部分是交叉是分别交叉两种基因然后switch基因直接完整继承一个亲本
5. 使用了离散多项式变异
6. oneshot模型训练，设置开关基因来决定每个cell是否参与训练（可参考）

## 2021-Evolutionary Neural Architecture Search for High-Dimensional Skip-Connection Structures on DenseNet Style Networks 
1. Furthermore, while many neural architecture search algorithms utilize performance estimation techniques to reduce computation time, empirical evaluations of these performance estimation techniques remain limited. 此外，尽管许多神经架构搜索算法利用性能估计技术来减少计算时间，但这些性能估计技术的经验评估仍然有限。 #局限-性能评估
3. 这项工作侧重于利用进化神经架构搜索来检查网络的搜索空间，这些网络遵循基本的DenseNet结构，但没有固定的跳过连接。
4. Genetic CNN was highly computationally expensive. #局限-复杂度高
7. The structures found by the algorithm are examined to shed light on the importance of different types of skip-connection structures in convolutional neural networks, including the discovery of a simple skip-connection removal, which improves DenseNet performance on CIFAR10.对算法发现的结构进行了检查，以阐明卷积神经网络中不同类型的跳过连接结构的重要性，包括发现了一种简单的跳过连接移除，这提高了CIFAR10上的DenseNet性能。
8. Crossover, shown in Algorithm 1, changes an individual  by choosing a random mate for it, and with probability  Pr(Crossover) swapping the bits of the individual to the corresponding value of the chosen mate. Thus, Pr(Crossover),  instead of controlling the probability that crossover occurs,  controls how much genetic material, on average, individuals  will take from a randomly selected mate如算法1所示，通过为个体选择随机配偶，并以概率Pr（Crossover）将个体的比特交换到所选配偶的对应值，从而改变个体。因此，Pr（交叉）不是控制交叉发生的概率，而是控制个体平均从随机选择的配偶身上获得多少遗传物质 注：直接改变个体的对应比特值，一定会对个体进行改变-没有太大借鉴意义，随机性很强 #创新-交叉 
9. Mutation, shown in Algorithm 2, randomly changes bits  of an individual with Pr(Mutate) chance to a value drawn  from the relevant entry in the initialization probability matrix.  Mutation, in this way, can be considered analogous to  crossover, but instead of making an individual more like  another network in the population, it becomes more like a  randomly initialized individual. This allows mutation to utilize prior information about the search space (codified in the  initialization probability matrix), while still providing diversity  to the population.如算法2所示，突变将具有Pr（突变）机会的个体的位随机改变为从初始化概率矩阵中的相关条目中提取的值。以这种方式，突变可以被视为类似于交叉，但它不会使个体更像群体中的另一个网络，而是变得更像一个随机初始化的个体。这允许变异利用关于搜索空间的先验信息（编码在初始化概率矩阵中），同时仍然为种群提供多样性。 注：类似于交叉的变异，是和初始个体进行交叉，但是可以生成种群中没有的新个体来增加了多样性。可以看出变异是为了增强种群多样性。我的论文可以在变异上顺着增加种群多样性的思路，目前思路是生成一个组件池，
### 总结
1. 使用的连接矩阵来表示跳跃连接

## 2021-Multiobjective Evolutionary Design of Deep  Convolutional Neural Networks for Image  Classification-多目标演化
1. 1) the obtained architectures are either  solely optimized for classification performance, or only for one  deployment scenario and 2) the search process requires vast  computational resources in most approaches.
   1）目前获得的架构仅仅为了分类精度，或者仅仅只是一种<mark>部署场景</mark>，2）搜索过程在大部分方法中都需要大量的计算资源 #优势-多场景 #局限-复杂度高
2. The flexibility provided from simultaneously obtaining multiple architecture choices for different compute requirements further differentiates our approach from other methods in the literature.
   同时获得不同计算要求的多个架构选择所提供的灵活性**进一步将我们的方法与文献中的其他方法区分开来。** #优势-多场景 #研究特异性-多目标NAS
1. The proposed method addresses the first shortcoming by populating a set of architectures to approximate the entire Pareto  frontier through genetic operations that recombine and modify  architectural components progressively.
   提出的方法通过遗传操作生成一系列接近帕累托前沿的架构，这种遗传算法逐步重组和修改架构组件 #优势-多目标NAS
2. <mark>Our approach improves  computational efficiency by </mark>carefully down-scaling the architectures during the search as well as reinforcing the patterns  commonly shared among past successful architectures through  Bayesian model learning.
   我们的方法通过在搜索过程中仔细缩小架构，并通过贝叶斯模型学习加强过去成功架构之间通常共享的模式来提高计算效率。 #
3.  One of the main driving forces behind this success is the introduction of many CNN architectures, including GoogLeNet [1], ResNet [2], DenseNet [3], etc., in the context of object classification. Concurrently, architecture designs,such as ShuffleNet [4], MobileNet [5], LBCNN [6], etc., have been developed with the goal of enabling real-world deployment of high-performance models on resource-constrained devices. These developments are the fruits of years of painstaking efforts and human ingenuity.这一成功背后的主要驱动力之一是在对象分类的背景下引入了许多CNN架构，包括GoogLeNet [1]，ResNet [2]，DenseNet [3]等。同时，架构设计，如ShuffleNet [4]，MobileNet [5]，LBCNN [6]等，已经开发出来，目标是在资源有限的设备上实现高性能模型的实际部署。这些发展是多年艰苦努力和人类聪明才智的成果。
4. Our proposed algorithm, NSGANetV1, is an iterative process in which initial architectures are made gradually better  as a group, called a population. In every iteration, a group  of offspring (i.e., new architectures) is created by applying variations through crossover and mutation to the more  promising of the architectures already found, also known as  parents, from the population. Every member in the population  (including both parents and offspring) compete for survival  and reproduction (becoming a parent) in each iteration. The  initial population may be generated randomly or guided by  prior-knowledge, i.e., seeding the past successful architectures  directly into the initial population. Subsequent to initialization,  NSGANetV1 conducts the search in two sequential stages:  1) exploration, with the goal of discovering diverse ways to  construct architectures and 2) exploitation that reinforces the  emerging patterns commonly shared among the architectures  successful during exploration. A set of architectures representing efficient tradeoffs between network performance and  complexity is obtained at the end of evolution, through genetic  operators and a Bayesian-model-based learning procedure. A  flowchart and a pseudocode outlining the overall approach are  shown in Fig. 1 and Algorithm 1 #介绍-NSGAII-NAS
5. Most existing evolutionary  NAS approaches [14], [19], [24], [32] search only one aspect  of the architecture space—e.g., the connections and/or hyperparameters.之前的工作只考虑一种空间 #局限-搜索空间
6. Most existing evolutionary  NAS approaches [14], [19], [24], [32] search only one aspect  of the architecture space—e.g., the connections and/or hyperparameters. In contrast, NSGANetV1 searches over both  operations and connections—the search space is thus more  comprehensive, including most of the previous successful  architectures designed both by human experts and algorithmically. 此研究不止一种搜索空间，有操作搜索和连接搜索两种 #创新-搜索空间
7. Given a population of architectures, parents are  selected from the population with a fitness bias. This choice  is dictated by two observations: 1) offspring created around  better parents are expected to have higher fitness on average  than those created around worse parents, with the assumption of some level of gradualism in the solution space and  2) occasionally (although not usually), offspring perform better than their parents, through inheriting useful traits from both  parents. Because of this, one might demand that the best architecture in the population should always be chosen as one of  the parents. However, the deterministic and greedy nature of  that approach would likely lead to premature convergence due  to the loss of diversity in the population [38].  #论文创新点参考-演化
8. To address this  problem, we use binary tournament selection [39] to promote  parent architectures in a stochastic fashion. At each iteration,  binary tournament selection randomly picks two architectures  from the population, then the one favored by the multiobjective  selection criterion described in Section III-B becomes one of  the parents. This process is repeated to select a second parent architecture; the two parent architectures then undergo a  crossover operation.使用二进制选择来随机化演化亲本的选择，这样不能准确的得到多样性的个体，专门做一个集合池，1. 里面是虽然帕累托前沿适应性不高，但是有潜力的组件 #论文创新点参考-演化 
9. In NSGANetV1, we use two types of crossover (with equal  probability of being chosen) to efficiently exchange substructures between two parent architectures. The first type is at  the block level, in which the offspring architectures are created by recombining the Normal block from the first parent  with the Reduction block from the other parent and vice  versa. The second type is at the node level, where a node  from one parent is randomly chosen and exchanged with  another node at the same position from the other parent. 使用了两种层次的交叉操作，块交叉（交叉双方亲本的Normal block或者reduction block，或者选择一个节点交叉）这是两种细粒度的交叉操作，我的不仅有块、（多个）节点、还有跳跃连接。
10. . Most existing evolutionary  NAS approaches [14], [19], [24], [32] search only one aspect  of the architecture space—e.g., the connections and/or hyperparameters.In contrast, NSGANetV1 searches over both  operations and connections—the search space is thus more  comprehensive, including most of the previous successful  architectures designed both by human experts and algorithmically. 不单单是超参数还是还是连接的搜索，是连接和搜索共同搜索的研究。  #论文创新点参考-演化
11. Among the many different NAS methods being continually proposed, evolutionary algorithms (EAs) are getting a  plethora of attention, due to their population-based nature and  flexibility in encoding.    在不断提出的许多不同的 NAS 方法中，进化算法 （EA） 因其基于群体的性质和编码的灵活性而受到大量关注。 #夸赞-演化算法
8.Therefore, to overcome this  computational bottleneck, we carefully (using a series of ablation studies) <mark>down-scale the architectures to create their proxy  models</mark> [8], [17], which can be optimized efficiently in the  lower-level through SGD. 
### 总结
1. 遗传部分的亮点是使用了贝叶斯模型计算历史中表现好的模型一些节点顺序之间的关系，来利用贝叶斯计算模块排列的概率（是否有节点继承） #创新-个体选择
2. 使用了两个层次的交叉，块层次和节点层次 #创新-交叉
3. 使用了基于NSGAII的架构搜索创新 #创新-NSGAII
4. CNN有network-level和block-level两种层次设计 #设计网络架构
5. 很多指标都可以做为某个目标代理，经过实验选择了flops作为代理
6. To simultaneously compare and select architectures based on these two objectives, we use the nondominated  ranking and the “crowded-ness” concepts proposed in [29]. #NAS-多目标演化
### 是否使用了权重继承


## 2022-GNN-EA: Graph Neural Network with Evolutionary  Algorithm
### 好表达
1. Unfortunately, existing graph NAS methods are usually susceptible to unscalable depth, redundant omputation, constrained search space and some other limitations.
   说明当前图神经网络架构搜索的限制
2. The experiment results show that GNN-EA exhibits comparable performance to the previous state-of-the-art handcrafted and automated GNN models.
   表达自己的图搜索的结果非常好
3. We present an evolutionary graph neural network architecture search strategy based on fine-grained atomic operations.
   表达出自己提出了一个基于什么的图网络搜索框架
4. Following the search strategy, we propose GNNEA, a framework that can achieve adaptive adjustment of neural structures without human intervention.
   这个搜索策略还不需要人的干预，结果很好
5. We conduct comparison experiments on five real-world datasets to evaluate our method. The results prove that GNN-EA outperforms the previous handcrafted GNN models and shows comparable performance to the state-of-the-art automated GNN models.
   我们做了实验，也写为了贡献之一
4. Table I shows the candidate set of atomic operations.
   介绍搜索空间
5. The new generation is composed of elite individuals and new individuals created by crossover and mutation operators.We iterate this process to maximize the fitness on a specific task and evolve desirable graph neural networks.
   新一代是怎么产生的，迭代的运行
### 总结
1. 提出两种（两个层面）交叉策略。
2. 使用两个层面的交叉操作
3. 对适应值最不好的两个个体变异
### 缺点
1. 评估花销大--结论处说明了这一点

##  2021-DE-GCN: Differential Evolution as an optimization algorithm for Graph Convolutional Networks
### 好表达
1. Neural networks had impressive results in recent years. Although neural networks only performed using Euclidean data in past decades, many data-sets in the real world have graph structures.This gap led researchers to implement deep learning on graphs.
   神经网络最近很成功，夸赞图
2. In recent years, learning with graphs and extracting latent information from networks is a hot research topic [1].
   近年来，图神经网络很火
3. Graphs’ data structure is more complicated than the Euclidian data structure. As a result, this leads to complicated learning process in graphs. Authors in [2] have formulated neural networks for graphs.
   介绍图复杂的特点
### 总结
1. 使用差分遗传优化（一种实数优化方法）优化图卷积神经网络的权重参数

## 2022-Auto-GNAS: A Parallel Graph Neural  Architecture Search Framework
### 表达
介绍部分：
1. In recent years,  graph neural networks (GNNs) have received extensive  attention from many researchers as an effective method for  mining the potential information of graph data [3], [4]. The  classic graph neural network models, including GCN [5] and GAT [6], have achieved good results in graph data mining. However, to obtain expected performance on a given  graph dataset, it is necessary to design the architecture of  graph neural networks based on the specific characteristics  of graph datasets, and it usually requires a lot of manual  work and domain expert experience.
   说明图网络应用广泛和搜索图神经架构的必要性
2. There are two types of GCNs, spectral-based [19], and spatial-based [20]. The spectral-based method needs to operate  on the entire graph. It is not easy to parallel and hardly scale  to big graphs. However, the spatial-based method is flexible  to aggregate feature information between neighbor nodes.
   说明基于频谱的图网络是使用有难度的
3. However, when we need to process a large-scale graph dataset,  such as the graph classification task in biological networks, it  will significantly increase the time cost of evaluating a GNN  architecture
   大规模的数据集，评估个体需要更多的时间花费
4. 文中描述搜索空间的部分可以借鉴，介绍搜索搜索空间，可以从图网络执行顺序上来介绍（如本文）
5. Each genetic searcher can simultaneously use information entropy and estimation feedback signal to constrain the search direction. 
   每个基因搜索者可以同时使用信息熵和估计反馈信号来约束搜索方向。
6. Each searcher  can simultaneously use the feedback information of GNN  architecture estimation and information entropy to accelerate the search process for getting better GNN architecture.
   每个搜索者可以同时使用GNN架构估计和信息熵的反馈信息来加速搜索过程，以获得更好的GNN架构。
### 好想法
1. The architecture mutation-selection probability vector P~is a soft constraint strategy to limit search direction. It can restrict search direction on the region near the GNN architectures with good performance and simultaneously reserve the probability of exploring other areas in the vast search space.
   架构突变选择概率向量P~是一种限制搜索方向的软约束策略。它可以以良好的性能限制GNN架构附近区域的搜索方向，同时保留在广阔的搜索空间中探索其他区域的概率。
2. With the analysis of GNN architecture features, we find  that different architecture component values have different frequencies of occurrence in the GNN architectures  that can get good performance on a given dataset.  Inspired by the theory that association algorithm mines  frequent itemset [23], we use information entropy to measure the correlation between GNN architecture component values and good performance.
   通过对GNN架构特征的分析，我们发现不同的架构组件值在GNN架构中具有不同的出现频率，可以在给定数据集上获得良好的性能。 受关联算法挖掘频繁项集[23]的理论启发，我们使用信息熵来衡量GNN架构组件值与良好性能之间的相关性。
   ### 总结
1. <mark>在本文中的导向型变异中只是考虑了某个组件在整个个体里出现的概率，是否可以继续考虑顺序关系，某个位置出现这个组件出现的概率。</mark>

## 2022-A Graph Architecture Search Method Based On  Grouped Operations
### 好表达
1. Generally, graph neural networks can be applied  to two categories of tasks, node-level tasks and graph-level  tasks. For node-level task, GNN models usually learn the  hidden representation HV 2 RjV j×d of nodes and then adopt  a predictor on node representation to complete the task [20].  For graph-level task, a representation for the whole graph is  learned to complete the task.
   介绍图网络节点层面和图层面的任务
2. we propose a graph architecture search method to decrease the instability with a large number of candidate operations. 消除搜索的不稳定性
3. We use a continuous relaxation of our search space and  optimize the hyper-networks with a gradient-based algorithm.我们使用搜索空间的持续松弛，并使用基于梯度的算法优化超网络。
### 总结
1. 只是搜索的聚合方式
## 2022-Android Malware Detection Using Supervised Deep Graph Representation Learning
### 好表达
1. There is an urgent demand  for developing malware detection techniques to deal with  the massive growth of Android malware.
   发展一项技术是如何必要
2. 
3.
4. Graph neural networks (GNNs) [4] are a popular and  flexible class of machine learning models that extend convolutional neural networks (CNNs) to graph-structured data  by facilitating the learning of relationships between graph  elements.
   介绍图神经网络
### 总结
1. <mark>缺点</mark>，只使用了一种读出方式，且读出没有用到图的结构信息 #缺点
2. 使用了两个自动编码器，分别得到两个一维向量，送入到mlp进行识别 #原理

## 2022-Residual Convolutional Graph Neural Network with Subgraph Attention Pooling
1. An obvious idea is to learn the detailed topology using  several graph convolution layers before each pooling  layer.  #原理 
   在池化前会使用卷积层学习节点特征
2. 和直接求和、平均，取最大不同的读出方式，提出了更加dedicated方式
3. residual架构
4. We propose a new strategy for graph-level representation generation, which separately aggregates each node’s information and introduces the attention mechanism to distinguish  the different contributions of each node to graph representation and alleviate the loss of critical and structural information.
   我们提出了一种新的图级表示生成策略，该策略分别聚合每个节点的信息，并引入注意力机制来区分每个节点对图表示的不同贡献，并减轻关键和结构信息的丢失。 #原理 
5. Pooling approaches are divided  into structure-based or feature-based methods. #GraphPooling
6. Structure-based approaches[13]  output a coarsened graph via clustering through the  convolutional layer. Feature-based approaches[6, 14, 15]  leverage the node features to give a score for each  node and then remove a part of the nodes based on  the scores. #GraphPooling 
7. 1-hop subgraph features are  used to replace node features to compute the attention  scores (node importance) for each node.
   使用1阶邻居子图替代只有自己的节点
8. We set the 1-hop subgraph and replaced the features  of nodes with the features of the 1-hop subgraphs. 使用一阶子图代替单独节点
9. We used the 1-hop subgraph  features instead of single-node features to compute  attention scores for node selection. The left half of  Eq. (3) represents the relative value of the central node  and the first-order neighbor feature. The right half of  Eq. (3) represents the mean values of all nodes in the  subgraph. #原理 
### 总结
1. 定义了新的池化操作
2. 采取了丢弃节点来进行池化，减少了特征信息
