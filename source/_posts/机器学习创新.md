#   2022-02-03

- Swin Transformer
  - 论文：在整个图片上打patch，每个patch之间做自注意力，这个注意力是在全图上做的全局注意力，带来的问题是计算复杂度随着图片尺寸的增加会图片尺寸的平方成比例，所以提出每个patch里做自注意力，每个patch的大小固定，那么计算的复杂度就是patch的大小，随着图片尺寸的增大，计算复杂度呈线性增长
  - 想法：是否可以整个车辆产生的报文看作是一张大图，每次选出来的一块（29X29）看作是一个patch

# 2022-02-06

- MAE
  - 论文：这里借鉴BERT使用Transfomer来对图片做掩码（完形填空），最终从剩下的块里学出图像特征，然后恢复。这里作者指出与BERT不同的是，在NLP领域做掩码，盖住的是一个完整的词，几乎是包含有完整的语义信息，图像的掩码恢复是更有难度的
  - 想法：在车辆报文如果做掩码遮住，和图像类似，遮住的是一部分，并不包含一定完整信息

