---
title: 论文1算法
top: false
cover: false
toc: true
mathjax: false
date: 2022-03-30 14:02:24
author:
img:
coverImg:
password:
summary:
tags:
categories:
---

# 配合一本实体笔记本

# 构建双网络协同优化的dataloader

## 20220330

- 从最初的一个样本预测下一个样本的模型这样的缺点：
  - 每个样本更新一次梯度，让优化器非常受单个样本的左右干扰，不利于收敛，同样速度会很慢
- 解决方案
  - 在强化学习预测网络的部分，输入batchsize个样本的状态向量，输出batchsize个样本的数据长度
  - 执行方案一：
    - 因为强化学习的batchsize样本是在存下来的经验里取到的，所以不需要特别地再构建dataloader，就把图神经网络的输出的特征向量串行输入到强化学习网络
    - 具体步骤
      1. 先随机生成batchsize个数据长度，然后数据预备器取出，送入图网络batchsize个数据进行梯度更新，并把生成的batchsize个卷积后的特征向量送入强化学习，依次循环
  - 执行方法二：
    - 把



# 模型的策略改进

## 20220330

### 图神经网络预训练

- 使用batchsize并且shuffle的模式进行训练



## 强化学习改进

### 20220403

1. 可以按一定的概率随机选择动作，增加灵活性
2. updata的次数可以适当增加
3. 如果同时几个action的分数值是相等的，不加修改程序每次都选择第一个动作，可以修改action的位置来调整
4.   对reward正则化 变为-1 ～ 1或其他
5. Prioritized 按重要度程度抽样  
6. 当reward高于baseline时增加采样几率，否则减少
7. action总是不变需要扣分，action变化是加分项
8. 根据domain knowledge 设计更多的 reward 机制，增加灵活性
9. 增加机器的好奇心

## reward 改进

- 如果错的多需要加大惩罚

### 20220409

- 老版本 Alpha Go一开始使用的是Behavior cloning 

# 图数据可视化

## 需求

1. 根据出入度的大小调整节点的大小
2. 根据边权重的大小调整边的粗细
3. 可以得到有向图的指向信息
4. 不同标签的节点标注不同的颜色
4. 把state normalize到一个尺度



# 遗传算法部分

## 图网络模型搜索

### 搜索空间

- [ ] 2022-04-20 14:23:02 是否把 add-self加入搜索空间  （目前没有）
- [ ] 2022-04-20 14:42:08 是否把 图卷积层内的 concat 作为搜索空间的一部分 （目前没有）
- [ ] 2022-04-29 21:32:32 是否把 图卷积的每一层的隐藏层作为搜索空间的一部分 （目前没有）
- [ ] 2022-05-01 16:03:17 是否把 训练的batchsize作为搜索空间搜索 （目前没有）



# 卷积部分

- 由于图像边界点卷积（边界点参与的计算次数少）问题，是否需要在生成的01二进制图卷积时对二进制图像进行padding
- 可否可以合并几位数据做加和在归一化然后做入侵识别
- 卷积实验可以对比一下VGG
- 2022-08-12 21:04:45 **把每个报文使用一个向量来表示**



# 强化学习

- 增加状态表示的信息
- 可视化输入到强化学习的向量，看是否发生了明显的变化
- 当奖励累计到一定的负值则重新开始
- Reward 归一化了

## 知识归纳

- PPO的前身是TRPO



## 局限

- 无法应对比较新的模式，可以玩当前游戏，无法适应新游戏--



# 数据集部分

## 数据集命名

### Car_Hacking_Challenge_Dataset_rev20Mar2021

```
datadir='../data/Car_Hacking_Challenge_Dataset_rev20Mar2021/0_Preliminary/
```

- MergeTrainSub_D
  - MergeTrainSub_D_0_1_2_sub_dataset
    - MergeTrainSub_D_0_1_2_sub_random_1_200_500_A
  - MergeTrainSub_D_0_1_2_sub_processed
    - MergeTrainSub_D_0_1_2_sub_processed/ps_10_nor_true_random_42/dataset_Di_true_200_500.p
    - MergeTrainSub_D_0_1_2_sub_processed/ps_10_nor_true_random_42/graphs_list_Di_200_500.p



# 程序并行运行部分

100个个体分为4份同时运行，每份串行执行25

每个个体运行的log信息重定向到各自log文件



# 程序Pymoo遗传部分

- 搜索空间共19位

  | 基因位数 | 基因位数 | 最小值 | 最大值 |                 基因含义解释                 |
  | :------: | :------: | :----: | :----: | :------------------------------------------: |
  |    0     |    1     |   0    |   1    |                 是否为有向图                 |
  |    1     |    1     |   0    |   1    |          图坍缩过程中是否使用正则化          |
  |   2-3    |    2     |   0    |   2    |       两个（图坍缩前后）图卷积块的深度       |
  |   4-5    |    2     |   0    |   2    |           预测层两个全连接层的比例           |
  |    6     |    1     |   0    |   5    |             神经网络Dropout参数              |
  |    7     |    1     |   0    |   3    |           神经网络WeightDecay参数            |
  |    8     |    1     |   0    |   3    |                神经网络学习率                |
  |   9-10   |    2     |   0    |   2    | 每个图卷积块内每次卷积输出特征向量的结合方式 |
  |  11-16   |    6     |   0    |   4    |        每个图网络输出后经过的激活函数        |
  |  17-18   |    2     |   0    |   2    |       两个可变连接层输出经过的激活函数       |
  |   合计   |    19    |        |        |                                              |

  

  



# 目标函数值

- params 
  - 最大：279862
  - 最小：22492

- flops
  - 最大: 113016384
  - 最小: 54379584

# 程序实行步骤

- 图网络部分
  - 先训练出超网络的参数
  - 使用训练出的个体训练mlp
  - 在训练mlp个体中选出优良个体，作为初始代
  - 每代个体 数量 60
  - 每个个体训练 20 epoch



# 0510任务

- 修改 实时获取数据程序 不需要shuffle 并且加上 D_1_2_sub
- 在程序结尾验证数据对齐 程序结束时查看 graph剩下的数据多少 conv_can数据集剩下多少，对比一下
- 一个一个batch的预测 因为最后graph形不成一个batch 就会结束 会浪费数据，先不管这么多了
- 重要任务，**获取两个网络结合之后的入侵检测精度**
- 主要需要解决和验证**两个数据集的对齐问题**
- 

# 差几张图

- 图神经网络的总览图
- 图神经网络的遗传优化图
- 卷积网络的遗传优化图



# 结果部分

- CNN模型 写成基因的形式 444 4 0 444 复杂度 2628714486
- LSTM模型 复杂度 57344
- DNN acc: 0.9665764762056342 复杂度 6817762
- GNN 复杂度 54379585
- 本模型 GNN+CNN 2683094071
- 全连接网络模型
- 优化的单独的CNN
- 优化的单独的GNN



- 五个模型的训练曲线 有无方向 和 有无正则化的 两种图线对比
- 两个模块的遗传算法部分的结果
- 表格展示结果



# LSTM

- 把D_1中的 ID的 220 部分 转换为数据集
- 结果
  - 使用ID220 D0训练 D1验证 精度 97% 0.9701388004584235



# DNN

- 这些功能的执行效率高、复杂度低，因为它们是通过网络直接从比特流生成的。该技术在离线训练特征的同时监测车辆网络中的交换数据包，并在我们的实验中提供了对攻击的实时响应，具有显著的高检测率。
- 今天的车辆系统嵌入了许多称为电子控制单元（ECU）的计算机设备来控制和监控子系统【1】。ECU通过车内网络相互通信，以促进先进的汽车应用，如carputer和自动驾驶服务。控制器局域网（CAN）[2]提供了一种简单可靠的通信协议，作为车辆网络的设计标准，不仅连接传感器和控制器，而且连接互联网。随着车对车（V2V）和车对基础设施（V2I）通信接口的出现，CAN的采用加快了应用程序的速度[3]。然而，车辆系统的开放性增加了恶意网络攻击的风险，这些攻击可能会严重损害人的生命。车辆网络中的保护机制关注新的安全威胁。



# 卷积遗传算法部分

- 交叉过后，有重复个体

- 2022-08-11 23:17:08 搜索损失函数



# 关于写作时的问题

1. 论文中只有图坍缩部分出现坍缩过程的公式，因为只有这里有公式所以就保留着了，但是这里坍缩的过程是参考的之前的论文

   Y. Ma, S. Wang, C. C. Aggarwal, and J. Tang, "Graph Convolutional Networks with EigenPooling," presented at the Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, Anchorage, AK, USA, 2019. [Online]. Available: https://doi.org/10.1145/3292500.3330982.

   我把这篇论文中的方法运用到了构造的报文图数据的图坍缩分类过程。

   我的问题是是否可以保留这里的公式？

2. 论文的





# Intro

## NAS

- 

## 图神经网络

- 遗传算法优化图网络
  最近，神经架构搜索（NAS）吸引了越来越多的研究兴趣[2]。如图1所示，GNNs体系结构中有许多组件，如注意、聚合和激活功能，它们构成了图形神经网络的搜索空间。架构搜索算法从搜索空间中对组件的组合进行采样𝑆 例如{𝑔 𝑎𝑡,𝑠𝑢𝑚,𝑡𝑎𝑛ℎ, ...} 在本例中，作为GNNs体系结构𝑠. 架构（architecture）𝑠 通过性能评估策略进一步评估。搜索算法使用估计的性能从搜索空间生成新的更好的GNNs体系结构𝑆. 最近，一些研究集中在图神经结构搜索问题上。





# 2022-08-18 修改论文

- 目前论文中使用到的演化算法部分是在神经架构搜索上，卷积网络，图神经网络架构搜索
- 增加其他的演化算法应用场景，并加入改进思想
- 演化算法是一种优化算法，求最优的算法
- 我的目的是得到最优的神经网络架构，每次采样的最优数据长度（强化学习解决）
- 在优化神经架构方面提出创新点

## 加入演化算法创新增加入侵检测率和降低模型复杂度

- 目前可以直接入手的改进是提升入侵识别精度（）---降低模型复杂度（）
- 更好的提取图特征和二进制图像特征
- 演化算法中的动态环境如何考虑



## 初始化子种群的创新

- 基于多个中心点初始化子种群




# 介绍部分

- 大体介绍图网络的应用和原理





# (多目标演化图神经)论文内容和参考

1. 搜索空间参考 2020-Neural Architecture Search in Graph Neural Networks
2. 添加跳跃连接  2020-AutoGraph Automated Graph Neural Network





# TEVC参考文献阅读
## 2022-Evolutionary Search for Complete Neural Network Architectures With Partial Weight Sharing
### 好想法+好表达
1. However, the full weight sharing training paradigm in OSNAS may result in strong interference across candidate architectures and mislead the architecture search.
2. The efﬁciency of performance estimation of the candidate architectures is therefore greatly enhanced because it avoids training a large number of candidate models from scratch.
   oneshot可以不用反复训练的优势
### 好表达
1. As a result, optimal networks obtained by Evo-OSNAS can achieve more competitive performance than the state-of-the-art networks found by existing NAS algorithms.
   夸赞自己算法
2. NAS can be viewed as a bilevel optimization problem and mathematically formulated as follows: train valid
   介绍NAS
3. RL-based NAS methods adopt a controller to sample a new candidate network to be trained and its performance is used as the reward score. Then, the reward score can be adopted to update the controller for sampling a better candidate network in the next iteration.
   介绍强化学习的NAS
4. The  benefit of the proposed mutation operation is to make the  one-shot model training less likely to get trapped in local minimums.
   夸赞变异
### 观点提取
1. oneshot模型训练，设置开关基因来决定每个cell是否参与训练（可参考）
### 总结
1. 虽然本文应用了oneshot，本文的搜索空间涉及不同的操作类型，<mark>每一层的都固定一种类型的的操作，每次选择时选中，则直接继承，从而实现oneshot，但是一开始无法训练所有情况的模块，则需要设计一个模块库。交叉我们也可以使用两阶段，GNN层面和layer层面。然后变异根据库内的情况，增加个体多样性。</mark>


### 总结
1. oneshot网络
2. 有结构基因和swich基因两种
3. 演化部分是交叉是分别交叉两种基因然后switch基因直接完整继承一个亲本
4. 使用了离散多项式变异


## 2021-Evolutionary Neural Architecture Search for High-Dimensional Skip-Connection Structures on DenseNet Style Networks
### 好表达
1. However, the use of neural architecture search for the discovery of skip-connection structures, an important element in modern convolutional neural networks, is limited within the literature.
   说明某项研究少
2. Genetic CNN was highly computationally expensive.
   某项研究复杂度高
3. how these networks are represented within the evolutionary NAS algorithm.
   这些网络如何在演化NAS中表示

## 2021-Multiobjective Evolutionary Design of Deep  Convolutional Neural Networks for Image  Classification-多目标演化
### 好想法+好表达
1. 1) the obtained architectures are either  solely optimized for classification performance, or only for one  deployment scenario and 2) the search process requires vast  computational resources in most approaches.
   1）目前获得的架构仅仅为了分类精度，或者仅仅只是一种部署场景，2）搜索过程在大部分方法中都需要大量的计算资源
2. The flexibility provided from simultaneously obtaining multiple architecture choices for different compute requirements further differentiates our approach from other methods in the literature.
   同时获得不同计算要求的多个架构选择所提供的灵活性**进一步将我们的方法与文献中的其他方法区分开来。**

### 好表达
1. The proposed method addresses the first shortcoming by populating a set of architectures to approximate the entire Pareto  frontier through genetic operations that recombine and modify  architectural components progressively.
   提出的方法通过遗传操作生成一系列接近帕累托前沿的架构，这种遗传算法逐步重组和修改架构组件
2. Our approach improves  computational efficiency by carefully down-scaling the architectures during the search as well as reinforcing the patterns  commonly shared among past successful architectures through  Bayesian model learning.
   我们的方法通过在搜索过程中仔细缩小架构，并通过贝叶斯模型学习加强过去成功架构之间通常共享的模式来提高计算效率。
3.  One of the main driving forces behind this success is the introduction of many CNN architectures, including GoogLeNet [1], ResNet [2], DenseNet [3], etc., in the context of object classification. Concurrently, architecture designs,such as ShuffleNet [4], MobileNet [5], LBCNN [6], etc., have been developed with the goal of enabling real-world deployment of high-performance models on resource-constrained devices. These developments are the fruits of years of painstaking efforts and human ingenuity.
   这一成功背后的主要驱动力之一是在对象分类的背景下引入了许多CNN架构，包括GoogLeNet [1]，ResNet [2]，DenseNet [3]等。同时，架构设计，如ShuffleNet [4]，MobileNet [5]，LBCNN [6]等，已经开发出来，目标是在资源有限的设备上实现高性能模型的实际部署。这些发展是多年艰苦努力和人类聪明才智的成果。
4. Among the many different NAS methods being continually proposed, evolutionary algorithms (EAs) are getting a  plethora of attention, due to their population-based nature and  flexibility in encoding.（**夸赞演化算法**）
   在不断提出的许多不同的 NAS 方法中，进化算法 （EA） 因其基于群体的性质和编码的灵活性而受到大量关注。
5. down-scale the architectures to create their proxy  models [8], [17], which can be optimized efficiently in the  lower-level through SGD.
   夸赞使用代理的重要性
### 总结
1. 遗传部分的亮点是使用了贝叶斯模型计算历史中表现好的模型一些节点顺序之间的关系，来利用



## 2022-GNN-EA: Graph Neural Network with Evolutionary  Algorithm
### 好表达
1. Unfortunately, existing graph NAS methods are usually susceptible to unscalable depth, redundant omputation, constrained search space and some other limitations.
   说明当前图神经网络架构搜索的限制
2. The experiment results show that GNN-EA exhibits comparable performance to the previous state-of-the-art handcrafted and automated GNN models.
   表达自己的图搜索的结果非常好
3. We present an evolutionary graph neural network architecture search strategy based on fine-grained atomic operations.
   表达出自己提出了一个基于什么的图网络搜索框架
4. Following the search strategy, we propose GNNEA, a framework that can achieve adaptive adjustment of neural structures without human intervention.
   这个搜索策略还不需要人的干预，结果很好
5. We conduct comparison experiments on five real-world datasets to evaluate our method. The results prove that GNN-EA outperforms the previous handcrafted GNN models and shows comparable performance to the state-of-the-art automated GNN models.
   我们做了实验，也写为了贡献之一
4. Table I shows the candidate set of atomic operations.
   介绍搜索空间
5. The new generation is composed of elite individuals and new individuals created by crossover and mutation operators.We iterate this process to maximize the fitness on a specific task and evolve desirable graph neural networks.
   新一代是怎么产生的，迭代的运行
### 总结
1. 提出两种（两个层面）交叉策略。
2. 使用两个层面的交叉操作
3. 对适应值最不好的两个个体变异
### 缺点
1. 评估花销大--结论处说明了这一点

##  2021-DE-GCN: Differential Evolution as an optimization algorithm for Graph Convolutional Networks
### 好表达
1. Neural networks had impressive results in recent years. Although neural networks only performed using Euclidean data in past decades, many data-sets in the real world have graph structures.This gap led researchers to implement deep learning on graphs.
   神经网络最近很成功，夸赞图
2. In recent years, learning with graphs and extracting latent information from networks is a hot research topic [1].
   近年来，图神经网络很火
3. Graphs’ data structure is more complicated than the Euclidian data structure. As a result, this leads to complicated learning process in graphs. Authors in [2] have formulated neural networks for graphs.
   介绍图复杂的特点

## 自己论文
1. 因为是第一次使用图神经网络做入侵检测领域的研究，初始种群找不到成功的案例，所以使用随机初始化的方法初始化初始种群。2021-Multiobjective Evolutionary
2. 如果两个亲本都是来自于好的个体，那么就会反复利用好的个体的组件，会不会导致早熟并且丢失多样性。2021-Multiobjective Evolutionary
3. 2022-Evolutionary Search和2021-Multiobjective Evolutionary都提出了新的交叉方式，2022是交叉结构基因或者switch基因，2021是块（reduction块和normal块）交叉或者节点交叉
4. 当模型训练到一定程度，可以使用模型来采样个体（先随机采样操作类型，然后使用模型来确定操作的先后顺序生成个体）2021-Multiobjective Evolutionary 自己可以加入双存档，论文中模型只是从好的组件得到贝叶斯采样模型，是否可以使用双存档来训练贝叶斯（双贝叶斯）。
5. 有residual，但是不是任意的residual 2022-GNN-EA: Graph Neural Network with Evolutionary  Algorithm